This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repopack on: 2024-10-16T01:04:35.334Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Repository structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repopack's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

Additional Info:
----------------

For more information about Repopack, visit: https://github.com/yamadashy/repopack

================================================================
Repository Structure
================================================================
templates/
  upload.html
.gitattributes
.gitignore
app.py
botify_segmentation.py
gunicorn.conf.py
package.json
README.md
render.yaml
requirements.txt

================================================================
Repository Files
================================================================

================
File: templates/upload.html
================
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Botify URL Analysis Tool</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
            background-color: #1e1e1e;
            color: #ffffff;
        }
        h1, h2, h3 { color: #bb86fc; }
        form { margin-bottom: 20px; }
        input[type="file"], input[type="text"], select {
            background-color: #2e2e2e;
            color: #ffffff;
            border: 1px solid #bb86fc;
            padding: 5px;
            margin-bottom: 10px;
            width: 100%;
        }
        button {
            background-color: #bb86fc;
            color: #1e1e1e;
            border: none;
            padding: 10px 20px;
            cursor: pointer;
            font-size: 16px;
            margin-right: 10px;
            margin-bottom: 10px;
        }
        button:hover { background-color: #9966cc; }
        #results { display: none; }
        #loading { display: none; color: #03dac6; }
        a { color: #03dac6; }
        pre { background-color: #2e2e2e; padding: 10px; border-radius: 5px; }
        .error { color: #cf6679; margin-bottom: 10px; }
    </style>
</head>
<body>
    <h1>Botify URL Analysis Tool</h1>
    <h2>Add in a CSV with a URL column to use the tool.</h2>
    <h3>Soft limit of 35mb per CSV with the tool.</h3>
    <form id="upload-form" enctype="multipart/form-data">
        <input type="text" name="client_name" placeholder="Client Name" required><br>
        <input type="file" name="file" accept=".csv" id="file-upload"><br>
        <select id="file-select" name="selected_file">
            <option value="">Select a previously uploaded file</option>
        </select><br>
        <button type="submit">Analyze URLs</button>
        <button type="button" id="refresh-files">Refresh File List</button>
    </form>
    <button id="delete-files">Delete All Files</button>
    <p><strong>Note:</strong> To refresh the deployment, use Render.com's deploy hooks or manual deploy option in the Render dashboard.</p>
    <div id="error" class="error"></div>
    <div id="loading">Analyzing... This may take a while for large datasets.</div>
    <div id="results"></div>

    <script>
        function loadFiles() {
            fetch('/list_files')
                .then(response => response.json())
                .then(files => {
                    const select = document.getElementById('file-select');
                    select.innerHTML = '<option value="">Select a previously uploaded file</option>';
                    files.forEach(file => {
                        const option = document.createElement('option');
                        option.value = file;
                        option.textContent = file;
                        select.appendChild(option);
                    });
                })
                .catch(error => {
                    console.error('Error loading files:', error);
                });
        }

        document.getElementById('refresh-files').addEventListener('click', loadFiles);

        document.getElementById('upload-form').addEventListener('submit', function(e) {
            e.preventDefault();
            var fileUpload = document.getElementById('file-upload');
            var fileSelect = document.getElementById('file-select');
            var errorDiv = document.getElementById('error');
            var loading = document.getElementById('loading');
            var results = document.getElementById('results');
            
            if (fileUpload.files.length === 0 && fileSelect.value === '') {
                errorDiv.textContent = 'Please either upload a new file or select an existing file.';
                return;
            }
            
            errorDiv.textContent = '';
            var formData = new FormData(this);
            
            loading.style.display = 'block';
            results.style.display = 'none';
            
            fetch('/', {
                method: 'POST',
                body: formData
            })
            .then(response => response.json())
            .then(data => {
                loading.style.display = 'none';
                if (data.error) {
                    errorDiv.textContent = 'Error: ' + data.error;
                    results.style.display = 'none';
                } else {
                    results.style.display = 'block';
                    var content = '<h2>Analysis Results</h2>';
                    content += '<p>Total URLs processed: <strong>' + data.total_urls_processed + '</strong></p>';
                    content += '<p><a href="/download/' + data.txt_file + '" target="_blank">Download TXT Results</a></p>';
                    content += '<p><a href="/download/' + data.csv_file + '" target="_blank">Download CSV Results (Ngram Analysis)</a></p>';
                    content += '<p><a href="/download/' + data.botify_file + '" target="_blank">Download Botify Segmentation Rules</a></p>';
                    content += '<p><a href="/download/' + data.markdown_file + '" target="_blank">Download All Segmentation Recommendations (Markdown)</a></p>';
                    content += '<h3>Insights:</h3><ul>';
                    data.insights.forEach(function(insight) {
                        content += '<li>' + insight + '</li>';
                    });
                    content += '</ul><h3>Top 10 Segmentation Suggestions:</h3><pre>';
                    data.segmentation_suggestions.forEach(function(suggestion) {
                        content += suggestion + '\n\n';
                    });
                    content += '</pre>';
                    content += '<h3>Top 10 Ngrams:</h3><ul>';
                    for (var ngram in data.top_ngrams) {
                        content += '<li>' + ngram + ': ' + data.top_ngrams[ngram] + '</li>';
                    }
                    content += '</ul>';
                    results.innerHTML = content;
                }
            })
            .catch(error => {
                loading.style.display = 'none';
                errorDiv.textContent = 'Error: ' + error;
                results.style.display = 'none';
            });
        });

        document.getElementById('delete-files').addEventListener('click', function() {
            if (confirm('Are you sure you want to delete all files? This action cannot be undone.')) {
                fetch('/delete_files', { method: 'POST' })
                    .then(response => response.json())
                    .then(data => {
                        alert(data.message);
                        // Refresh the file list after deletion
                        loadFiles();
                    })
                    .catch(error => {
                        console.error('Error:', error);
                        alert('An error occurred while deleting files.');
                    });
            }
        });

        // Load files when the page loads
        loadFiles();
    </script>
</body>
</html>

================
File: .gitattributes
================
# Auto detect text files and perform LF normalization
* text=auto

================
File: .gitignore
================
results

================
File: app.py
================
import re
from collections import Counter
from urllib.parse import urlparse, parse_qs
import traceback
from datetime import datetime
import asyncio
import aiohttp
import pandas as pd
import csv
import json
from flask import Flask, request, jsonify, render_template, send_file, Response
from werkzeug.utils import secure_filename
import os
import shutil
import logging
import io
from botify_segmentation import generate_botify_segmentation, export_botify_segmentation, export_segmentation_markdown

app = Flask(__name__)
logging.basicConfig(level=logging.INFO)

UPLOAD_FOLDER = 'uploads'
RESULTS_FOLDER = 'results'
ALLOWED_EXTENSIONS = {'csv'}
CHUNK_SIZE = 1000  # Process URLs in chunks of 1000

app.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER
app.config['RESULTS_FOLDER'] = RESULTS_FOLDER
os.makedirs(UPLOAD_FOLDER, exist_ok=True)
os.makedirs(RESULTS_FOLDER, exist_ok=True)

def allowed_file(filename):
    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS

def find_url_columns(df):
    url_columns = [col for col in df.columns if 'url' in col.lower()]
    if not url_columns:
        app.logger.warning(f"No URL column found. Columns in DataFrame: {df.columns.tolist()}")
    return url_columns

def process_csv_chunk(chunk):
    url_columns = find_url_columns(chunk)
    if not url_columns:
        return None
    url_column = url_columns[0]
    urls = chunk[url_column].dropna().tolist()
    return urls

def read_csv_with_custom_header(file_path):
    with open(file_path, 'r') as f:
        first_line = f.readline().strip()
        if first_line.startswith('sep='):
            separator = first_line[-1]
            df = pd.read_csv(file_path, sep=separator, skiprows=[0])
        else:
            df = pd.read_csv(file_path)
    return df

def stream_csv(file_path):
    df = read_csv_with_custom_header(file_path)
    for i in range(0, len(df), CHUNK_SIZE):
        yield df[i:i+CHUNK_SIZE]

async def analyze_url(url):
    try:
        parsed_url = urlparse(url)
        
        results = {
            'subdomain': '',
            'domain': '',
            'path': parsed_url.path,
            'path_without_params': parsed_url.path.split(';')[0].split('?')[0],
            'query_params': list(parse_qs(parsed_url.query).keys()),
            'file_extension': '',
            'segments': [],
            'protocol': parsed_url.scheme,
            'path_length': len(parsed_url.path.split('/')),
            'query_param_count': len(parse_qs(parsed_url.query))
        }
        
        if parsed_url.netloc:
            domain_parts = parsed_url.netloc.split('.')
            if len(domain_parts) > 2:
                results['subdomain'] = '.'.join(domain_parts[:-2])
            results['domain'] = '.'.join(domain_parts[-2:])
        
        if '.' in parsed_url.path.split('/')[-1]:
            results['file_extension'] = parsed_url.path.split('/')[-1].split('.')[-1]
        
        results['segments'] = [f"{seg}" for seg in parsed_url.path.split('/') if seg]
        
        return results
    except Exception as e:
        app.logger.error(f"Error processing URL {url}: {str(e)}")
        return None

async def analyze_urls_chunk(urls):
    async with aiohttp.ClientSession() as session:
        tasks = [analyze_url(url) for url in urls]
        results = await asyncio.gather(*tasks)
    return [r for r in results if r is not None]

def merge_results(chunk_results):
    merged = {
        'subdomains': Counter(),
        'domains': Counter(),
        'paths': Counter(),
        'paths_without_params': Counter(),
        'query_params': Counter(),
        'file_extensions': Counter(),
        'segments': Counter(),
        'protocol': Counter(),
        'path_length': Counter(),
        'query_param_count': Counter(),
    }
    
    for result in chunk_results:
        merged['subdomains'][result['subdomain']] += 1
        merged['domains'][result['domain']] += 1
        merged['paths'][result['path']] += 1
        merged['paths_without_params'][result['path_without_params']] += 1
        merged['query_params'].update(result['query_params'])
        if result['file_extension']:
            merged['file_extensions'][result['file_extension']] += 1
        merged['segments'].update(result['segments'])
        merged['protocol'][result['protocol']] += 1
        merged['path_length'][result['path_length']] += 1
        merged['query_param_count'][result['query_param_count']] += 1
    
    return merged

def generate_insights(analysis_results):
    insights = []
    
    total_urls = sum(analysis_results['domains'].values())
    insights.append(f"Total URLs analyzed: {total_urls:,}")
    
    if analysis_results['protocol']:
        insights.append(f"Most common protocol: {analysis_results['protocol'].most_common(1)[0][0]}")
    
    if analysis_results['subdomains']:
        insights.append(f"Most common subdomain: {analysis_results['subdomains'].most_common(1)[0][0]}")
    
    if analysis_results['domains']:
        insights.append(f"Most common domain: {analysis_results['domains'].most_common(1)[0][0]}")
    
    if analysis_results['paths']:
        insights.append(f"Most common path: {analysis_results['paths'].most_common(1)[0][0]}")
    
    if analysis_results['paths_without_params']:
        insights.append(f"Most common path without parameters: {analysis_results['paths_without_params'].most_common(1)[0][0]}")
    
    if analysis_results['query_params']:
        insights.append(f"Most common query parameter: {analysis_results['query_params'].most_common(1)[0][0]}")
    
    if analysis_results['file_extensions']:
        insights.append(f"Most common file extension: {analysis_results['file_extensions'].most_common(1)[0][0]}")
    
    if total_urls > 0:
        avg_path_depth = sum(k*v for k,v in analysis_results['path_length'].items()) / total_urls
        insights.append(f"Average path depth: {avg_path_depth:.2f}")
        
        avg_query_params = sum(k*v for k,v in analysis_results['query_param_count'].items()) / total_urls
        insights.append(f"Average number of query parameters: {avg_query_params:.2f}")
    
    return insights

def ngram_analysis(urls, n=2, min_count=5):
    ngrams = Counter()
    for url in urls:
        parsed_url = urlparse(url)
        path_parts = parsed_url.path.split('/')
        for i in range(len(path_parts) - n + 1):
            ngram = '/'.join(path_parts[i:i+n])
            ngrams[ngram] += 1
    
    return {ngram: count for ngram, count in ngrams.items() if count >= min_count}

async def process_urls(urls):
    chunk_results = []
    for i in range(0, len(urls), CHUNK_SIZE):
        chunk = urls[i:i+CHUNK_SIZE]
        chunk_result = await analyze_urls_chunk(chunk)
        chunk_results.extend(chunk_result)
        
        # Yield control to allow other tasks to run
        await asyncio.sleep(0)
    
    analysis_results = merge_results(chunk_results)
    insights = generate_insights(analysis_results)
    ngrams = ngram_analysis(urls)
    
    return {
        'analysis': analysis_results,
        'insights': insights,
        'ngrams': ngrams
    }

def process_urls_sync(urls):
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    results = loop.run_until_complete(process_urls(urls))
    loop.close()
    
    # Generate Botify segmentation
    botify_segmentation, all_segments = generate_botify_segmentation(urls)
    results['botify_segmentation'] = botify_segmentation
    results['all_segments'] = all_segments
    
    return results

def export_results(result, format, client_name):
    date_str = datetime.now().strftime("%Y%m%d")
    filename_base = f"url_analysis_{client_name}_{date_str}"
    
    if format == 'txt':
        filename = f"{filename_base}.txt"
        file_path = os.path.join(app.config['RESULTS_FOLDER'], filename)
        with open(file_path, 'w') as f:
            f.write("URL Analysis Results\n\n")
            f.write("Insights:\n")
            for insight in result['insights']:
                f.write(f"- {insight}\n")
            f.write("\nSegmentation Suggestions:\n")
            for suggestion in result['segmentation_suggestions']:
                f.write(f"{suggestion}\n\n")
            f.write("Botify Segmentation Rules:\n")
            f.write(result['botify_segmentation'])
            f.write("\n\nFull Analysis:\n")
            for key, value in result['analysis'].items():
                f.write(f"{key}:\n")
                for item, count in value.most_common(20):  # Limit to top 20 for readability
                    f.write(f"  {item}: {count:,}\n")
                f.write("\n")
            f.write("Ngram Analysis:\n")
            for ngram, count in sorted(result['ngrams'].items(), key=lambda x: x[1], reverse=True)[:20]:
                f.write(f"  {ngram}: {count:,}\n")
    elif format == 'csv':
        filename = f"{filename_base}.csv"
        file_path = os.path.join(app.config['RESULTS_FOLDER'], filename)
        df = pd.DataFrame([(k, v) for k, v in result['ngrams'].items()], columns=['Ngram', 'Count'])
        df = df.sort_values('Count', ascending=False)
        df.to_csv(file_path, index=False)
    
    # Export Botify segmentation rules
    botify_filename = f"botify_segmentation_{client_name}_{date_str}.txt"
    botify_file_path = os.path.join(app.config['RESULTS_FOLDER'], botify_filename)
    export_botify_segmentation(result['botify_segmentation'], botify_file_path)
    
    # Export all segmentation recommendations as markdown
    markdown_filename = f"all_segmentation_{client_name}_{date_str}.md"
    markdown_file_path = os.path.join(app.config['RESULTS_FOLDER'], markdown_filename)
    export_segmentation_markdown(result['all_segments'], markdown_file_path)
    
    return filename, botify_filename, markdown_filename

@app.route('/', methods=['GET', 'POST'])
def upload_file():
    if request.method == 'POST':
        try:
            start_time = datetime.now()
            client_name = request.form.get('client_name', 'unnamed_client')
            
            file_path = None
            
            if 'file' in request.files and request.files['file'].filename != '':
                file = request.files['file']
                if file and allowed_file(file.filename):
                    filename = secure_filename(file.filename)
                    file_path = os.path.join(app.config['UPLOAD_FOLDER'], filename)
                    file.save(file_path)
            elif 'selected_file' in request.form and request.form['selected_file'] != '':
                filename = request.form['selected_file']
                file_path = os.path.join(app.config['UPLOAD_FOLDER'], filename)
            
            if not file_path:
                return jsonify({'error': 'No file selected or uploaded'}), 400
            
            all_urls = []
            for chunk in stream_csv(file_path):
                urls = process_csv_chunk(chunk)
                if urls:
                    all_urls.extend(urls)
            
            results = process_urls_sync(all_urls)
            
            insights = results['insights']
            segmentation_suggestions = [f"@{segment}\npath */{segment}/*" for segment, _ in results['analysis']['segments'].most_common(10)]
            
            results['segmentation_suggestions'] = segmentation_suggestions
            
            txt_file, botify_file, markdown_file = export_results(results, 'txt', client_name)
            csv_file, _, _ = export_results(results, 'csv', client_name)
            
            end_time = datetime.now()
            processing_time = (end_time - start_time).total_seconds()
            
            response_data = {
                'message': 'Analysis complete',
                'txt_file': txt_file,
                'csv_file': csv_file,
                'botify_file': botify_file,
                'markdown_file': markdown_file,
                'insights': insights,
                'segmentation_suggestions': segmentation_suggestions,
                'top_ngrams': {k: f"{v:,}" for k, v in sorted(results['ngrams'].items(), key=lambda x: x[1], reverse=True)[:10]},
                'processing_time_seconds': processing_time,
                'file_size_mb': os.path.getsize(file_path) / (1024 * 1024),
                'total_urls_processed': f"{len(all_urls):,}"
            }
            
            return jsonify(response_data)
        
        except Exception as e:
            app.logger.error(f"Error processing file: {str(e)}")
            app.logger.error(traceback.format_exc())
            return jsonify({'error': f"Error processing file: {str(e)}"}), 500
    
    return render_template('upload.html')

@app.route('/list_files', methods=['GET'])
def list_files():
    files = [f for f in os.listdir(UPLOAD_FOLDER) if allowed_file(f)]
    return jsonify(files)

@app.route('/download/<filename>')
def download_file(filename):
    return send_file(os.path.join(app.config['RESULTS_FOLDER'], filename), as_attachment=True)

@app.route('/delete_files', methods=['POST'])
def delete_files():
    try:
        # Delete all files in the uploads folder except for sample.csv
        for filename in os.listdir(UPLOAD_FOLDER):
            if filename != 'sample-pagelist.csv':
                file_path = os.path.join(UPLOAD_FOLDER, filename)
                if os.path.isfile(file_path):
                    os.remove(file_path)

        # Delete all files in the results folder
        for filename in os.listdir(RESULTS_FOLDER):
            file_path = os.path.join(RESULTS_FOLDER, filename)
            if os.path.isfile(file_path):
                os.remove(file_path)

        return jsonify({'message': 'All files deleted successfully'}), 200
    except Exception as e:
        app.logger.error(f"Error deleting files: {str(e)}")
        return jsonify({'error': f"Error deleting files: {str(e)}"}), 500

def is_development():
    return not os.environ.get('FLASK_ENV') == 'production'

if __name__ == '__main__':
    port = int(os.environ.get('PORT', 10000))
    app.run(host='0.0.0.0', port=port)

================
File: botify_segmentation.py
================
from collections import Counter
from urllib.parse import urlparse
import os

def generate_botify_segmentation(urls, top_n=10):
    segments = Counter()
    all_segments = []
    
    for url in urls:
        parsed_url = urlparse(url)
        path_parts = parsed_url.path.strip('/').split('/')
        
        for i, part in enumerate(path_parts, 1):
            segment = f"{part}"
            segments[segment] += 1
            all_segments.append((i, segment))
    
    top_segments = segments.most_common(top_n)
    
    botify_rules = ["[segment:auto_generated]"]
    
    for segment, count in top_segments:
        rule = f"@{segment}\npath */{segment}/*"
        botify_rules.append(rule)
    
    return "\n\n".join(botify_rules), all_segments

def export_botify_segmentation(segmentation_rules, file_path):
    with open(file_path, 'w') as f:
        f.write(segmentation_rules)
    return os.path.basename(file_path)

def export_segmentation_markdown(all_segments, file_path):
    segment_levels = {}
    for level, segment in all_segments:
        if level not in segment_levels:
            segment_levels[level] = set()
        segment_levels[level].add(segment)
    
    with open(file_path, 'w') as f:
        f.write("# Segmentation Recommendations\n\n")
        for level in sorted(segment_levels.keys()):
            f.write(f"## Level {level}\n\n")
            for segment in sorted(segment_levels[level]):
                f.write(f"```\n@{segment}\npath */{segment}/*\n```\n\n")
    
    return os.path.basename(file_path)

================
File: gunicorn.conf.py
================
workers = 2
worker_class = 'sync'
timeout = 300  # 5 minutes
max_requests = 1000
max_requests_jitter = 50
graceful_timeout = 30  # Give workers 30 seconds to finish their current request before forcefully restarting

================
File: package.json
================
{
  "name": "url-analysis-tool",
  "version": "1.0.0",
  "description": "A tool to analyze URLs from CSV files, providing insights, segmentation suggestions, and ngram analysis",
  "main": "app.py",
  "scripts": {
    "start": "gunicorn app:app -c gunicorn.conf.py",
    "dev": "python app.py"
  },
  "repository": {
    "type": "git",
    "url": "https://github.com/yourusername/url-analysis-tool.git"
  },
  "keywords": [
    "url-analysis",
    "seo",
    "python",
    "flask"
  ],
  "author": "Your Name",
  "license": "MIT",
  "engines": {
    "python": ">=3.7.0"
  }
}

================
File: README.md
================
# URL Analysis Tool

This tool analyzes URLs from CSV files, providing insights, segmentation suggestions, and ngram analysis.

## Installation

1. Ensure you have Python 3.7+ installed on your system.

2. Clone this repository:
   ```
   git clone https://github.com/yourusername/url-analysis-tool.git
   cd url-analysis-tool
   ```

3. Create a virtual environment (optional but recommended):
   ```
   python -m venv venv
   source venv/bin/activate  # On Windows, use `venv\Scripts\activate`
   ```

4. Install the required packages:
   ```
   pip install flask pandas aiohttp tqdm
   ```

## Setup

1. Ensure you have the following folder structure in your project directory:
   ```
   url-analysis-tool/
   ├── app.py
   ├── templates/
   │   └── upload.html
   ├── uploads/
   └── results/
   ```

2. If the `uploads` and `results` folders don't exist, create them:
   ```
   mkdir uploads results
   ```

## Running the Application

1. From the project directory, run:
   ```
   python app.py
   ```

2. Open a web browser and go to `http://localhost:5000`.

## Usage

1. Enter a client name in the provided field.

2. Either upload a new CSV file or select a previously uploaded file from the dropdown.

3. Click "Analyze URLs" to start the analysis.

4. Once complete, you can view the results on the page and download JSON, TXT, and CSV files with the full analysis.

5. The generated files will be saved in the `results` folder.

## CSV File Format

The tool looks for any column containing "URL" in its name (case-insensitive). Examples of valid column names include:
- URL
- URLs
- Full URL
- Canonical URL

Ensure your CSV file has at least one column with "URL" in its name.

If your CSV file uses a custom separator, you can specify it in the first line of the file like this:
```
sep=,
Full URL
https://www.example.com/
...
```

## Features

- Flexible URL column detection
- Support for custom CSV separators
- Ngram analysis of URL patterns
- Segmentation suggestions
- Insights on domain, subdomain, and path distributions
- Export results in JSON, TXT, and CSV formats
- Local file management (upload and select previously uploaded files)

## Troubleshooting

If you encounter any issues:

1. Ensure all required packages are installed.
2. Check that the `templates`, `uploads`, and `results` folders exist and have the correct permissions.
3. Verify that your CSV file contains a column with "URL" in its name.
4. Check the Flask application logs for any error messages.
5. Ensure you have write permissions for both the `uploads` and `results` folders.
6. If download links don't work, verify that the files are being created in the `results` folder after analysis.

For further assistance, please open an issue on the GitHub repository.

## Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## License

This project is licensed under the MIT License - see the LICENSE file for details.

================
File: render.yaml
================
services:
  - type: web
    name: url-analysis-tool-test
    env: python
    buildCommand: pip install -r requirements.txt
    startCommand: gunicorn app:app -c gunicorn.conf.py
    envVars:
      - key: PYTHON_VERSION
        value: 3.9.2
    disk:
      name: uploads
      mountPath: /opt/render/project/src/uploads
      sizeGB: 1
    headers:
      - path: /*
        name: Content-Security-Policy
        value: "upgrade-insecure-requests"
    httpHeaders:
      - path: /*
        name: X-Frame-Options
        value: DENY
    # Add this line to increase the maximum upload size to 300MB
    maxUploadSize: 300

================
File: requirements.txt
================
Flask
pandas
aiohttp
gunicorn
