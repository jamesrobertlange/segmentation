This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repopack on: 2024-10-14T16:14:22.920Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Repository structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repopack's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

Additional Info:
----------------

For more information about Repopack, visit: https://github.com/yamadashy/repopack

================================================================
Repository Structure
================================================================
templates/
  upload.html
.gitattributes
app.py
README.md

================================================================
Repository Files
================================================================

================
File: templates/upload.html
================
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>URL Analysis Tool</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
            background-color: #1e1e1e;
            color: #ffffff;
        }
        h1, h2, h3 { color: #bb86fc; }
        form { margin-bottom: 20px; }
        input[type="file"], input[type="text"], select {
            background-color: #2e2e2e;
            color: #ffffff;
            border: 1px solid #bb86fc;
            padding: 5px;
            margin-bottom: 10px;
            width: 100%;
        }
        button {
            background-color: #bb86fc;
            color: #1e1e1e;
            border: none;
            padding: 10px 20px;
            cursor: pointer;
            font-size: 16px;
            margin-right: 10px;
        }
        button:hover { background-color: #9966cc; }
        #results { display: none; }
        #loading { display: none; color: #03dac6; }
        a { color: #03dac6; }
        pre { background-color: #2e2e2e; padding: 10px; border-radius: 5px; }
        .error { color: #cf6679; margin-bottom: 10px; }
    </style>
</head>
<body>
    <h1>URL Analysis Tool</h1>
    <form id="upload-form" enctype="multipart/form-data">
        <input type="text" name="client_name" placeholder="Client Name" required><br>
        <input type="file" name="file" accept=".csv" id="file-upload"><br>
        <select id="file-select" name="selected_file">
            <option value="">Select a previously uploaded file</option>
        </select><br>
        <button type="submit">Analyze URLs</button>
        <button type="button" id="refresh-files">Refresh File List</button>
    </form>
    <div id="error" class="error"></div>
    <div id="loading">Analyzing... This may take a while for large datasets.</div>
    <div id="results"></div>

    <script>
        function loadFiles() {
            fetch('/list_files')
                .then(response => response.json())
                .then(files => {
                    const select = document.getElementById('file-select');
                    select.innerHTML = '<option value="">Select a previously uploaded file</option>';
                    files.forEach(file => {
                        const option = document.createElement('option');
                        option.value = file;
                        option.textContent = file;
                        select.appendChild(option);
                    });
                })
                .catch(error => {
                    console.error('Error loading files:', error);
                });
        }

        document.getElementById('refresh-files').addEventListener('click', loadFiles);

        document.getElementById('upload-form').addEventListener('submit', function(e) {
            e.preventDefault();
            var fileUpload = document.getElementById('file-upload');
            var fileSelect = document.getElementById('file-select');
            var errorDiv = document.getElementById('error');
            var loading = document.getElementById('loading');
            var results = document.getElementById('results');
            
            if (fileUpload.files.length === 0 && fileSelect.value === '') {
                errorDiv.textContent = 'Please either upload a new file or select an existing file.';
                return;
            }
            
            errorDiv.textContent = '';
            var formData = new FormData(this);
            
            loading.style.display = 'block';
            results.style.display = 'none';
            
            fetch('/', {
                method: 'POST',
                body: formData
            })
            .then(response => response.json())
            .then(data => {
                loading.style.display = 'none';
                if (data.error) {
                    errorDiv.textContent = 'Error: ' + data.error;
                    results.style.display = 'none';
                } else {
                    results.style.display = 'block';
                    var content = '<h2>Analysis Results</h2>';
                    content += '<p>URL column used: <strong>' + data.url_column_used + '</strong></p>';
                    content += '<p><a href="/download/' + data.json_file + '" target="_blank">Download JSON Results</a></p>';
                    content += '<p><a href="/download/' + data.txt_file + '" target="_blank">Download TXT Results</a></p>';
                    content += '<p><a href="/download/' + data.csv_file + '" target="_blank">Download CSV Results (Ngram Analysis)</a></p>';
                    content += '<h3>Insights:</h3><ul>';
                    data.insights.forEach(function(insight) {
                        content += '<li>' + insight + '</li>';
                    });
                    content += '</ul><h3>Top 5 Segmentation Suggestions:</h3><pre>';
                    data.segmentation_suggestions.forEach(function(suggestion) {
                        content += suggestion + '\n\n';
                    });
                    content += '</pre>';
                    content += '<h3>Top 10 Ngrams:</h3><ul>';
                    for (var ngram in data.top_ngrams) {
                        content += '<li>' + ngram + ': ' + data.top_ngrams[ngram] + '</li>';
                    }
                    content += '</ul>';
                    results.innerHTML = content;
                }
            })
            .catch(error => {
                loading.style.display = 'none';
                errorDiv.textContent = 'Error: ' + error;
                results.style.display = 'none';
            });
        });

        // Load files when the page loads
        loadFiles();
    </script>
</body>
</html>

================
File: .gitattributes
================
# Auto detect text files and perform LF normalization
* text=auto

================
File: app.py
================
import re
from collections import Counter
from urllib.parse import urlparse, parse_qs
import traceback
import json
from datetime import datetime
import asyncio
import aiohttp
import pandas as pd
import csv
from flask import Flask, request, jsonify, render_template, send_file
from werkzeug.utils import secure_filename
import os
from tqdm import tqdm

app = Flask(__name__)

UPLOAD_FOLDER = 'uploads'
RESULTS_FOLDER = 'results'
ALLOWED_EXTENSIONS = {'csv'}
CHUNK_SIZE = 10000  # Process URLs in chunks of 10000

app.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER
app.config['RESULTS_FOLDER'] = RESULTS_FOLDER
os.makedirs(UPLOAD_FOLDER, exist_ok=True)
os.makedirs(RESULTS_FOLDER, exist_ok=True)

def allowed_file(filename):
    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS

def find_url_columns(df):
    url_columns = [col for col in df.columns if 'url' in col.lower()]
    app.logger.info(f"Columns in DataFrame: {df.columns.tolist()}")
    app.logger.info(f"URL columns found: {url_columns}")
    return url_columns

def read_csv_with_custom_header(file_path):
    with open(file_path, 'r') as f:
        first_line = f.readline().strip()
        if first_line.startswith('sep='):
            separator = first_line[-1]
            df = pd.read_csv(file_path, sep=separator, skiprows=[0])
        else:
            df = pd.read_csv(file_path)
    return df

async def analyze_url(url, session):
    try:
        parsed_url = urlparse(url)
        
        results = {
            'subdomain': '',
            'domain': '',
            'path': parsed_url.path,
            'path_without_params': parsed_url.path.split(';')[0].split('?')[0],
            'query_params': list(parse_qs(parsed_url.query).keys()),
            'file_extension': '',
            'segments': [],
            'protocol': parsed_url.scheme,
            'path_length': len(parsed_url.path.split('/')),
            'query_param_count': len(parse_qs(parsed_url.query))
        }
        
        domain_parts = parsed_url.netloc.split('.')
        if len(domain_parts) > 2:
            results['subdomain'] = '.'.join(domain_parts[:-2])
        results['domain'] = '.'.join(domain_parts[-2:])
        
        if '.' in parsed_url.path.split('/')[-1]:
            results['file_extension'] = parsed_url.path.split('/')[-1].split('.')[-1]
        
        results['segments'] = [f"level_{i+1}:{seg}" for i, seg in enumerate(parsed_url.path.split('/')) if seg]
        
        return results
    except Exception as e:
        print(f"Error processing URL {url}: {str(e)}")
        return None

async def analyze_urls_chunk(urls):
    async with aiohttp.ClientSession() as session:
        tasks = [analyze_url(url, session) for url in urls]
        results = await asyncio.gather(*tasks)
    return [r for r in results if r is not None]

def merge_results(chunk_results):
    merged = {
        'subdomains': Counter(),
        'domains': Counter(),
        'paths': Counter(),
        'paths_without_params': Counter(),
        'query_params': Counter(),
        'file_extensions': Counter(),
        'segments': Counter(),
        'protocol': Counter(),
        'path_length': Counter(),
        'query_param_count': Counter(),
    }
    
    for result in chunk_results:
        merged['subdomains'][result['subdomain']] += 1
        merged['domains'][result['domain']] += 1
        merged['paths'][result['path']] += 1
        merged['paths_without_params'][result['path_without_params']] += 1
        merged['query_params'].update(result['query_params'])
        if result['file_extension']:
            merged['file_extensions'][result['file_extension']] += 1
        merged['segments'].update(result['segments'])
        merged['protocol'][result['protocol']] += 1
        merged['path_length'][result['path_length']] += 1
        merged['query_param_count'][result['query_param_count']] += 1
    
    return merged

def generate_insights(analysis_results):
    insights = []
    
    total_urls = sum(analysis_results['domains'].values())
    insights.append(f"Total URLs analyzed: {total_urls}")
    insights.append(f"Most common protocol: {analysis_results['protocol'].most_common(1)[0][0]}")
    
    if analysis_results['subdomains']:
        insights.append(f"Most common subdomain: {analysis_results['subdomains'].most_common(1)[0][0]}")
    
    insights.append(f"Most common domain: {analysis_results['domains'].most_common(1)[0][0]}")
    insights.append(f"Most common path: {analysis_results['paths'].most_common(1)[0][0]}")
    insights.append(f"Most common path without parameters: {analysis_results['paths_without_params'].most_common(1)[0][0]}")
    
    if analysis_results['query_params']:
        insights.append(f"Most common query parameter: {analysis_results['query_params'].most_common(1)[0][0]}")
    
    if analysis_results['file_extensions']:
        insights.append(f"Most common file extension: {analysis_results['file_extensions'].most_common(1)[0][0]}")
    
    avg_path_depth = sum(k*v for k,v in analysis_results['path_length'].items()) / total_urls
    insights.append(f"Average path depth: {avg_path_depth:.2f}")
    
    avg_query_params = sum(k*v for k,v in analysis_results['query_param_count'].items()) / total_urls
    insights.append(f"Average number of query parameters: {avg_query_params:.2f}")
    
    segmentation_suggestions = []
    for segment, count in analysis_results['segments'].most_common(10):
        level, value = segment.split(':')
        segmentation_suggestions.append(f"@{value}\npath */{value}/*")
    
    return insights, segmentation_suggestions

def ngram_analysis(urls, n=2, min_count=5):
    ngrams = Counter()
    for url in urls:
        parsed_url = urlparse(url)
        path_parts = parsed_url.path.split('/')
        for i in range(len(path_parts) - n + 1):
            ngram = '/'.join(path_parts[i:i+n])
            ngrams[ngram] += 1
    
    return {ngram: count for ngram, count in ngrams.items() if count >= min_count}

async def main(urls):
    try:
        chunk_results = []
        for i in tqdm(range(0, len(urls), CHUNK_SIZE), desc="Processing URL chunks"):
            chunk = urls[i:i+CHUNK_SIZE]
            chunk_result = await analyze_urls_chunk(chunk)
            chunk_results.extend(chunk_result)
        
        analysis_results = merge_results(chunk_results)
        insights, segmentation_suggestions = generate_insights(analysis_results)
        
        ngrams = ngram_analysis(urls)
        
        return {
            'analysis': analysis_results,
            'insights': insights,
            'segmentation_suggestions': segmentation_suggestions,
            'ngrams': ngrams
        }
    except Exception as e:
        return {
            'error': str(e),
            'traceback': traceback.format_exc()
        }

def export_results(result, format, client_name):
    date_str = datetime.now().strftime("%Y%m%d")
    filename_base = f"url_analysis_{client_name}_{date_str}"
    
    if format == 'json':
        filename = f"{filename_base}.json"
        file_path = os.path.join(app.config['RESULTS_FOLDER'], filename)
        with open(file_path, 'w') as f:
            json.dump(result, f, indent=2, default=lambda x: list(x.items()) if isinstance(x, Counter) else x)
    elif format == 'txt':
        filename = f"{filename_base}.txt"
        file_path = os.path.join(app.config['RESULTS_FOLDER'], filename)
        with open(file_path, 'w') as f:
            f.write("URL Analysis Results\n\n")
            f.write("Insights:\n")
            for insight in result['insights']:
                f.write(f"- {insight}\n")
            f.write("\nSegmentation Suggestions:\n")
            for suggestion in result['segmentation_suggestions']:
                f.write(f"{suggestion}\n\n")
            f.write("Full Analysis:\n")
            for key, value in result['analysis'].items():
                f.write(f"{key}:\n")
                for item, count in value.most_common(20):  # Limit to top 20 for readability
                    f.write(f"  {item}: {count}\n")
                f.write("\n")
            f.write("Ngram Analysis:\n")
            for ngram, count in sorted(result['ngrams'].items(), key=lambda x: x[1], reverse=True):
                f.write(f"  {ngram}: {count}\n")
    elif format == 'csv':
        filename = f"{filename_base}.csv"
        file_path = os.path.join(app.config['RESULTS_FOLDER'], filename)
        df = pd.DataFrame([(k, v) for k, v in result['ngrams'].items()], columns=['Ngram', 'Count'])
        df = df.sort_values('Count', ascending=False)
        df.to_csv(file_path, index=False)
    
    return filename

def find_url_columns(df):
    return [col for col in df.columns if 'url' in col.lower()]

@app.route('/list_files', methods=['GET'])
def list_files():
    files = [f for f in os.listdir(UPLOAD_FOLDER) if allowed_file(f)]
    return jsonify(files)

@app.route('/', methods=['GET', 'POST'])
def upload_file():
    if request.method == 'POST':
        client_name = request.form.get('client_name', 'unnamed_client')
        
        file_path = None
        
        if 'file' in request.files and request.files['file'].filename != '':
            file = request.files['file']
            if file and allowed_file(file.filename):
                filename = secure_filename(file.filename)
                file_path = os.path.join(app.config['UPLOAD_FOLDER'], filename)
                file.save(file_path)
        elif 'selected_file' in request.form and request.form['selected_file'] != '':
            filename = request.form['selected_file']
            file_path = os.path.join(app.config['UPLOAD_FOLDER'], filename)
        
        if not file_path:
            return jsonify({'error': 'No file selected or uploaded'})
        
        # Process the file
        try:
            df = read_csv_with_custom_header(file_path)
            app.logger.info(f"CSV file read successfully. Shape: {df.shape}")
            url_columns = find_url_columns(df)
            
            if not url_columns:
                return jsonify({'error': f"No column containing 'URL' found in the CSV file. Columns found: {', '.join(df.columns.tolist())}"})
            
            # If multiple URL columns are found, use the first one
            url_column = url_columns[0]
            urls = df[url_column].dropna().tolist()
            
            # Run the analysis
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            results = loop.run_until_complete(main(urls))
            
            # Export results
            json_file = export_results(results, 'json', client_name)
            txt_file = export_results(results, 'txt', client_name)
            csv_file = export_results(results, 'csv', client_name)
            
            return jsonify({
                'message': 'Analysis complete',
                'json_file': json_file,
                'txt_file': txt_file,
                'csv_file': csv_file,
                'insights': results['insights'],
                'segmentation_suggestions': results['segmentation_suggestions'][:5],
                'top_ngrams': dict(sorted(results['ngrams'].items(), key=lambda x: x[1], reverse=True)[:10]),
                'url_column_used': url_column
            })
        except Exception as e:
            app.logger.error(f"Error processing file: {str(e)}")
            app.logger.error(traceback.format_exc())
            return jsonify({'error': f"Error processing file: {str(e)}"})
    
    return render_template('upload.html')

@app.route('/download/<filename>')
def download_file(filename):
    return send_file(os.path.join(app.config['RESULTS_FOLDER'], filename), as_attachment=True)

if __name__ == '__main__':
    app.run(debug=True)

================
File: README.md
================
# URL Analysis Tool

This tool analyzes URLs from CSV files, providing insights, segmentation suggestions, and ngram analysis.

## Installation

1. Ensure you have Python 3.7+ installed on your system.

2. Clone this repository:
   ```
   git clone https://github.com/yourusername/url-analysis-tool.git
   cd url-analysis-tool
   ```

3. Create a virtual environment (optional but recommended):
   ```
   python -m venv venv
   source venv/bin/activate  # On Windows, use `venv\Scripts\activate`
   ```

4. Install the required packages:
   ```
   pip install flask pandas aiohttp tqdm
   ```

## Setup

1. Ensure you have the following folder structure in your project directory:
   ```
   url-analysis-tool/
   ├── app.py
   ├── templates/
   │   └── upload.html
   ├── uploads/
   └── results/
   ```

2. If the `uploads` and `results` folders don't exist, create them:
   ```
   mkdir uploads results
   ```

## Running the Application

1. From the project directory, run:
   ```
   python app.py
   ```

2. Open a web browser and go to `http://localhost:5000`.

## Usage

1. Enter a client name in the provided field.

2. Either upload a new CSV file or select a previously uploaded file from the dropdown.

3. Click "Analyze URLs" to start the analysis.

4. Once complete, you can view the results on the page and download JSON, TXT, and CSV files with the full analysis.

5. The generated files will be saved in the `results` folder.

## CSV File Format

The tool looks for any column containing "URL" in its name (case-insensitive). Examples of valid column names include:
- URL
- URLs
- Full URL
- Canonical URL

Ensure your CSV file has at least one column with "URL" in its name.

If your CSV file uses a custom separator, you can specify it in the first line of the file like this:
```
sep=,
Full URL
https://www.example.com/
...
```

## Features

- Flexible URL column detection
- Support for custom CSV separators
- Ngram analysis of URL patterns
- Segmentation suggestions
- Insights on domain, subdomain, and path distributions
- Export results in JSON, TXT, and CSV formats
- Local file management (upload and select previously uploaded files)

## Troubleshooting

If you encounter any issues:

1. Ensure all required packages are installed.
2. Check that the `templates`, `uploads`, and `results` folders exist and have the correct permissions.
3. Verify that your CSV file contains a column with "URL" in its name.
4. Check the Flask application logs for any error messages.
5. Ensure you have write permissions for both the `uploads` and `results` folders.
6. If download links don't work, verify that the files are being created in the `results` folder after analysis.

For further assistance, please open an issue on the GitHub repository.

## Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## License

This project is licensed under the MIT License - see the LICENSE file for details.
